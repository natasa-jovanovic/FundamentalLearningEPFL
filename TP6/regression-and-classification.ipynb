{"cells":[{"cell_type":"markdown","metadata":{"id":"roGVKmtmaGSK"},"source":["# EE-411 Fundamentals of inference and learning, EPFL \n","## Exercise Session 6: Ridge Regression and Lasso on real datasets\n","\n","In this sixth set of exercises, we will see how to implement the methods introduced in the last exercise session and discussed in the last lecture to work with some real datasets. We will see which are the differences between these regularized least-square methods and the standard OLS solution.\n","\n","**What you will learn today:**  In this sixth notebook, we shall see how to use scikit-learn to implement Ridge and Lasso. We also introduce `pipelines`, an important tool which is frequently used in Machine Learning. Furthermore, we will linger on the important concept of sparsity."]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"id":"-Eg-9Ps6aAwW"},"outputs":[],"source":["#We import the usual packages\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"dto2wKg95Y-y"},"source":["# A) Ozone\n","---\n","Let us look at a particular real-life data problem. In the following example we take the prediction of ozone concentration as a factor of other weather-based features. As with all data problems, it behooves us to take a look at all of the information that we have about the dataset.\n","\n","#### Dataset loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1634739267733,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"IwmAl9MS6PgU","outputId":"daa16105-4109-4986-cb6b-5360b72158ef"},"outputs":[],"source":["data =   pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/LAozone.data')\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"A8tnJPM35Y-z"},"source":["Alright, we're ready to get started ! Now, before we touch anything, we need to follow best practices. When faced with a new dataset, we need to split it into three parts: **Training**(and within that, **Validation**), and **Testing** sets. \n","\n","The best practice here is to take the test data and lock it away somewhere. It is always tempting to tune your algorithms to give the best test performance. However, even if the regression isn't explicitly *trained* on the test data, as practitioners, we could be continually making changes in an effort to get our numbers up.\n","\n","Instead, we should deep-freeze the test data, and then tune as much as we can via **cross-validation (CV)** on our training data.\n","\n","#### Train/Test splitting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1181,"status":"ok","timestamp":1634739268908,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"oQyEsGDt5Y-z","outputId":"e1537ca5-9595-4664-f8d0-b9bc07e03672"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","#--- Convert from DataFrame to NDArray ---#\n","# We also ensure that we load in all data as floating point values\n","# so that we don't miss anything to truncations.\n","y = data['ozone'].to_numpy().astype(float)\n","X = data[data.columns[1:]].to_numpy()\n","\n","#--- Split Dataset ---#\n","XTrain, XTest, yTrain, yTest = train_test_split(X,y,test_size = 0.25, random_state=1)\n","\n","print(\"Training Samples: \", len(yTrain))\n","print(\"Testing  Samples: \", len(yTest))"]},{"cell_type":"markdown","metadata":{"id":"XxP_xg625Y-z"},"source":["Now, before we start attempting to fit models, lets take a bit of care and apply some pre-processing to our dataset. The de-facto pre-processing is *centering and normalization*. Specifically, many flavors of estimators (OLS, RR, etc.) can be thrown off by large differences in scale and variations between the features. We can easily account for this in our estimators by simply normalizing the feature columns and removing averages. Scikit-Learn has some features to do this automatically!\n","\n","#### Preprocessing"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"hlyrRvAQ5Y-0"},"outputs":[],"source":["from sklearn import preprocessing\n","\n","#--- Scaling ---#\n","scaler = preprocessing.StandardScaler().fit(XTrain)\n","XTest_scaled = scaler.transform(XTest)\n","XTrain_scaled = scaler.transform(XTrain)\n","\n","#--- Center observations (simplify the OLS)---#\n","yTrain_mean = np.mean(yTrain)\n","yTrain_centered = yTrain - yTrain_mean"]},{"cell_type":"markdown","metadata":{"id":"Cu6wR7tr5Y-0"},"source":["Now it is time for us to choose our estimator. What should we choose? "]},{"cell_type":"markdown","metadata":{"id":"XJY7nEEq5Y-0"},"source":["## Attempt 1: ordinary least squares (OLS)\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EBOVGZ58TeGC"},"source":["If you remember last exercise session, the equation to solve is of the type\n","\n","$$\n","(X^T X) \\hat {\\mathbf w} = X^T {\\mathbf y}\n","$$\n","\n","The function `linalg.solve(a,b)` can be used to solve a system of linear scalar equations $ax=b$, thus we can use it to compute the prediction of the model both on the train and on the test datasets"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"cMYULuaMZlTq"},"outputs":[],"source":["#--- Compute the OLS Estimate ---#\n","regOLS = np.linalg.solve(np.dot(XTrain_scaled.T,XTrain_scaled), np.dot(XTrain_scaled.T,yTrain))\n","\n","yp = np.dot(XTrain_scaled, regOLS)\n","ypTest = np.dot(XTest_scaled, regOLS)"]},{"cell_type":"markdown","metadata":{"id":"Ol32ddnsZmKS"},"source":["Let's see now how well the prediction performance looks and which are the learned parameters corresponding to the OLS solution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545},"executionInfo":{"elapsed":1088,"status":"ok","timestamp":1634739270682,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"imRZjEBS5Y-0","outputId":"0d57c940-1437-4017-9579-b04050a43842"},"outputs":[],"source":["#--- Visualize ---#\n","plt.figure(figsize=(14,7))\n","plt.subplot(121)\n","plt.plot(yTrain, yp + yTrain_mean,'.', label='Training') #remember to add again the mean when you plot\n","plt.plot(yTest, ypTest + yTrain_mean, label='Testing', marker='x')\n","plt.plot([0, 40], [0, 40], '-k', linewidth=0.7, label=\"Perfect\")\n","# plt.axis([0, 40, 0, 40])\n","plt.xlabel('Ozone (True)', fontsize=16)\n","plt.ylabel('Ozone (Predicted)', fontsize=16)\n","plt.legend(loc=2, fontsize=16)\n","plt.title('Prediction Performance', fontsize=18)\n","# Plot the learned model\n","plt.subplot(122)\n","plt.stem(regOLS)\n","plt.title('Learned Model $\\\\hat{\\\\beta}$', fontsize=18)\n","plt.xticks(range(9),data.keys()[1:10], rotation='vertical');"]},{"cell_type":"markdown","metadata":{"id":"dS8icHBvax9I"},"source":["Finally, to see how well the model is performing, we can compute the Residual Sum of Squares (RSS) for both the train and the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":389,"status":"ok","timestamp":1634739274204,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"QeMhKtMPaxc-","outputId":"ffb52e56-8a9e-44d8-8345-58c97240e7d5"},"outputs":[],"source":["#--- Print RSS ---#\n","rss_train = np.mean(np.power(yTrain_centered - yp,2))\n","rss_test = np.mean(np.power(yTest - yTrain_mean - ypTest,2))\n","print(\"Normalized RSS (Train): %0.2f\" % rss_train)\n","print(\"Normalized RSS  (Test): %0.2f\" % rss_test)"]},{"cell_type":"markdown","metadata":{"id":"59UtPKSScN4-"},"source":["The same thing could have been done using the packages present in **Scikit-Learn**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"z-Ac4GBccNrW"},"outputs":[],"source":["from sklearn import linear_model\n","\n","#--- Define OLS Estimator ---#\n","regL = linear_model.LinearRegression()\n","#--- Fit the parameters ---#\n","regL.fit(XTrain_scaled,yTrain_centered)\n","#--- Use the learned coefficients to predict the y ---#\n","yp_L = regL.predict(XTrain)\n","ypTest_L = regL.predict(XTest)"]},{"cell_type":"markdown","metadata":{"id":"Nag9kam2sXKe"},"source":["**Note:** the standard value `fit_intercept=True` in `LinearRegression`, but also in `Ridge` and in `Lasso`, allows us not to think about the intercept, since it is automatically considered by the method."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vWNK9T1euPgd"},"source":["#### 1) Compare these estimates to the preivious ones, computed without considering the intercept. Why they are so similar?\n"," - Is the intercept fitted by `LinearRegression` big? \n"," - What's the difference in the MSE of the two prediction?\n"," - (bonus) Make a plot!"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 2) Prove that fitting the intercept (as `LinearRegression`) is equivalent to what we did (subtracting the mean). (Hint: remember that the inpunts are scaled!)"]},{"cell_type":"markdown","metadata":{"id":"jXF5xTlL5Y-1"},"source":["## Attempt 2: Ridge Regression\n","---"]},{"cell_type":"markdown","metadata":{"id":"8-Jnh6PZb9VU"},"source":["For RR, that we implemented by hand in the last notebook, we can use directly Scikit-learn. First of all, we import as always the model, choosing for now the regularization constant $\\alpha = 0.01$"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"0PYMdFRI5Y-1"},"outputs":[],"source":["#--- Define Regression Estimator ---#\n","regRR = linear_model.Ridge(alpha=0.01)"]},{"cell_type":"markdown","metadata":{"id":"WjLPItMs5Y-1"},"source":["Now, we need to perform some kind of cross-validation (CV) to find the best set of parameters for our model. We will do this by constructing a **Pipeline**. A pipeline is a useful way of handling pre-processing on separate data partitions when performing CV. Let's take a look at that.\n","\n","In the next cell we use the pipeline to do two things:\n","\n","* We standardize features by removing the mean and scaling to unit variance (it is done by `preprocessing.StandardScaler()`)\n","\n","* We implement the fitting strategy; namely Ridge Regression\n","\n","#### Pipeline"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"csCkghRKNz6n"},"outputs":[],"source":["from sklearn.pipeline import make_pipeline \n","\n","#--- Make a Pre-processing + Fitting Pipeline ---#\n","pipe_regRR = make_pipeline(preprocessing.StandardScaler(), regRR)"]},{"cell_type":"markdown","metadata":{"id":"8-iK6g6WgGi5"},"source":["After that, we have  to define a Scoring Metric. In this case, to compare the fits we look at the prediction error via the RSS."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"2pDzNtPVgK9a"},"outputs":[],"source":["#--- Define a Scoring Metric (Residual Sum of Squares) ---#\n","def neg_rss(reg, X, y):\n","    yp = reg.predict(X)\n","    return -np.mean(np.power(y - yp,2))"]},{"cell_type":"markdown","metadata":{"id":"DeqdYNurgk0h"},"source":[" We can then create an iterator which performs a set of **randomized splits on the dataset into \"train\" and \"validation\"**. \n"," \n","Obviously, we have a trade-off between the test set size and the number of splits we should perform."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"8RJertM25Y-1"},"outputs":[],"source":["from sklearn.model_selection import ShuffleSplit\n","from sklearn.model_selection import GridSearchCV\n","\n","#--- Define CV Splitting ---#\n","cv = ShuffleSplit(n_splits=30, test_size=0.05, random_state=1)\n","\n","#--- Define the Parameters to Search ---#\n","param_grid = [\n","    {'ridge__alpha': np.logspace(-5,3,100)} # we do CV for values of alpha between 0.0001 and 1000\n","]"]},{"cell_type":"markdown","metadata":{"id":"3qbVB4sWjyqL"},"source":["Finally, we use `GradSearchCV` to find which is the best value of the parameter $\\alpha$"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3910,"status":"ok","timestamp":1634739745888,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"sQaDNH1ujzRa","outputId":"eeb902f9-d129-43d2-f02a-4ecdb236deef"},"outputs":[],"source":["#--- Define CV ---#\n","cv_regRR = GridSearchCV(pipe_regRR, param_grid, scoring=neg_rss, cv=cv,return_train_score=True) #Note that we need to use the RSS with a minus sign!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#--- Run CV ---#\n","cv_regRR.fit(XTrain,yTrain)"]},{"cell_type":"markdown","metadata":{"id":"Grg5pkob5Y-2"},"source":["Great! Now lets take a look at the performance of our estimator. Here, for example of Ridge regression, we chart over the $\\alpha$ parameter that we perform CV against."]},{"cell_type":"markdown","metadata":{"id":"iQyf1VWNihF7"},"source":["First of all, we save in `cvOptAlpha` the optimal regularization parameter $\\alpha$ found by the grid search, and in `cvOptParams` the corresponding parameters"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"MmKc8a9ojjft"},"outputs":[],"source":["#--- Record CV Optimizing Hyper-Params ---#\n","cvOptAlpha = cv_regRR.cv_results_['param_ridge__alpha'][cv_regRR.best_index_]\n","cvOptParams = cv_regRR.best_estimator_['ridge'].coef_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cvOptAlpha"]},{"cell_type":"markdown","metadata":{"id":"NuZLOf_qjkaH"},"source":["Then, we can plot the performance versus the regularization parameter $\\alpha$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"executionInfo":{"elapsed":872,"status":"ok","timestamp":1634739746754,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"5vsg98jK5Y-2","outputId":"f5206632-c172-4e5f-df38-839a6406ba52"},"outputs":[],"source":["#--- Visualize ---#\n","testedAlpha = cv_regRR.cv_results_['param_ridge__alpha']\n","trainScores = -cv_regRR.cv_results_['mean_train_score']   # Reverse Sign\n","testScores = -cv_regRR.cv_results_['mean_test_score']    # Reverse Sign\n","\n","plt.figure(figsize=(15,5))\n","plt.plot(testedAlpha, trainScores, '-', label='Training (Avg.)')\n","plt.plot(testedAlpha, testScores, '-', label='Validation (Avg.)')\n","plt.xlabel('Regularization Parameter $\\\\alpha$', fontsize=16)\n","plt.ylabel('$\\\\frac{1}{N}RSS$', fontsize=16)\n","plt.axvline(cvOptAlpha, label='$\\\\alpha^*$', color='k', linestyle=':')\n","plt.xscale('log')\n","plt.xlim([0.9*1e-3, 1e3])\n","plt.yscale('log')\n","plt.tight_layout()\n","plt.legend(loc=2, fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"OnaZo0435Y-2"},"source":["Now, lets take a look at how well we are able to make our predictions on the training set using this CV optimal value of $\\alpha$."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":451},"executionInfo":{"elapsed":847,"status":"ok","timestamp":1634739752763,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"5vdoamgR5Y-2","outputId":"06a1e6dc-fcd4-46d8-f056-86b626b859cf"},"outputs":[],"source":["#--- Get Training Predicitions ---#\n","pipe_regRR.set_params(ridge__alpha=cvOptAlpha) # !!! how to pass parameters to pipelines\n","pipe_regRR.fit(XTrain, yTrain)\n","yp = pipe_regRR.predict(XTrain)\n","\n","#--- Visualize ---#\n","plt.figure(figsize=(7,7))\n","plt.plot(yTrain, yp, '.', label='Training (CV-Opt)')\n","plt.plot([0, 40], [0, 40], '-k', linewidth=0.7, label=\"Perfect\")\n","# plt.axis([0, 40, 0, 40])\n","plt.xlabel('Ozone (True)', fontsize=16)\n","plt.ylabel('Ozone (Predicted)', fontsize=16)\n","plt.legend(loc=2, fontsize=16);"]},{"cell_type":"markdown","metadata":{"id":"njk7XIGi5Y-2"},"source":["And now, finally, we are ready to take our test data out of deep-freeze. How did we do?"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eLHgg4e9lHf8"},"source":["#### 3) Plot, as we previously did for OLS, the prediction performance and the learned parameters. After having done that, compute the RSS and compare it to the OLS one"]},{"cell_type":"markdown","metadata":{"id":"v9QgqxJx5Y-3"},"source":["## Attempt 3: Lasso\n","---\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"E6fOj_gVl0Xl"},"source":["### 4) Your turn: using the `linear_model.Lasso` estimator present in Scikit-Learn,  repeat the steps done for Ridge Regression:\n","* Create the pipeline for preprocessing + regressor\n","* Do the Grid Search to find the optimal regularization parameter\n","* Visualize how the performance varies when we change the regularization parameter \n","* Plot the prediction performance and the learned parameters. What do you observe?\n","* Compute the RSS and compare it to the RR and the OLS one."]},{"cell_type":"markdown","metadata":{"id":"vV1MU1qZba_h"},"source":["# B) Diabetes\n","\n","We will now perform some numerical experiments with the Diabetes Dataset trying to predict diabetes outcomes one year forward. This is a classical dataset in statistics, and more information can be found at <a href=\"https://archive.ics.uci.edu/ml/datasets/Diabetes\">https://archive.ics.uci.edu/ml/datasets/Diabetes</a>. Here is what we have: 10 baseline variables: age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of $n = 442$ diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n","\n","Our first goal is to plot the weights for each value of $\\lambda$ for Ridge Regression and LASSO. This is called a **regularization path**."]},{"cell_type":"markdown","metadata":{"id":"YX-15tDj3ZJr"},"source":["We first load the dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k3ditaL76wm8"},"source":["### 5) Using the pipelines `pipe_regRR` and `pipe_regLasso` constructed before for the Ozone problem:\n","\n","* Divide the dataset such that you have 150 training samples and 50 test samples\n","* Do the Grid Search to find the optimal regularization parameters, for $\\alpha$ in $[10^{-2},10^5]$\n","* Visualize how the performance varies when we change the regularization parameter, comparing Rindge and Lasso\n","\n","### 6) Then aswer to these questions [EVALUATED]:\n","- What do the points $\\alpha=0$  and  $\\alpha = +\\infty$  correspond to? Why for these values both the regularizations give the same result?\n","\n","- What is the qualitative difference between the LASSO path and Ridge Path? (Plot it as well in linear scale)"]},{"cell_type":"markdown","metadata":{},"source":["# Classification with Logistic regression and SVM "]},{"cell_type":"markdown","metadata":{},"source":["**What you will learn today**: You will learn how to implement a classifier. First we will analyze a toy example in which we will use a logistic classifier coded from scratch. Once we understood the logic behind the algorithm we will use the black box Sklearn one for recognizing hand-written digits. You will also implement SVM classifier on a different dataset to get familiar with this important family of algorithm."]},{"cell_type":"markdown","metadata":{},"source":["# 1) Logistic regression from scratch"]},{"cell_type":"markdown","metadata":{},"source":["Let us generate a synthetic dataset using a multivariate Gaussian distribution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.random.seed(0)\n","num_observations = 500\n","\n","x1 = np.random.multivariate_normal([0, 0], [[1, .5],[.5, 1]], num_observations)\n","x2 = np.random.multivariate_normal([1, 4], [[1, .8],[.8, 1]], num_observations)\n","\n","dataset = np.vstack((x1, x2)).astype(np.float32)\n","labels = np.hstack((np.zeros(num_observations),np.ones(num_observations)))\n","\n","dataset[:5]"]},{"cell_type":"markdown","metadata":{},"source":["Let's plot our data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(14,8))\n","plt.xlim(-3,4)\n","plt.scatter(dataset[:, 0], dataset[:, 1],c = labels, alpha = .4)"]},{"cell_type":"markdown","metadata":{},"source":["Our goal is to use a logistic function to fit our dataset. In this case:\n","$$P_{\\rm model}(y_i|{\\vec w} \\cdot {\\vec x_i}) = \\frac {e^{({\\vec w} \\cdot {\\vec x_i})y_i}}{1+\\exp({\\vec w} \\cdot {\\vec x_i})}$$\n","such that\n","$$P_{\\rm model}(y_i=1)  = \\frac {\\exp{({\\vec w} \\cdot {\\vec x_i})}}{1+\\exp({\\vec w} \\cdot {\\vec x_i})}  = \\frac {1}{1+\\exp(-{\\vec w} \\cdot {\\vec x_i})}~~~ \\text{and}~~~ P_{\\rm model}(y_i=0)  = \\frac {1}{1+\\exp({\\vec w} \\cdot {\\vec x_i})}  = \\frac {\\exp{(-{\\vec w} \\cdot {\\vec x_i})}}{1+\\exp(-{\\vec w} \\cdot {\\vec x_i})}$$\n","\n","\n","Now, we can either use the cross-entropy loss and write:\n","$$\\mathcal{L}(\\vec{w}) = - \\sum_{i=1}^n \\sum_{y_i=0,1} y_i \\log P_{\\rm model}(y_i) +  (1-y_i) \\log (1 - P_{\\rm model}(y_i)) $$\n","or equivalently write a maximum-likelihood problem with\n","$$\n","\\mathcal{L}(\\vec{w}|\\vec{\\bm{x}},\\bm{y}) \\propto \\sum_i \\log P_{\\rm model}(y_i|{\\vec w} \\cdot {\\vec x_i})\n","$$\n","In both cases, the problem boils down to minimizing the following loss:\n","$$\\mathcal{L}(\\vec {w}) =  \\sum_{i=1}^n - y_i {\\vec {w}} \\cdot {\\vec x}_i  + \\log{(1+\\exp({\\vec w} \\cdot {\\vec x}_i ))} $$\n","\n","Let us implement these function:"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def sigmoid(scores):\n","    return 1 / (1 + np.exp(-scores))\n","def log_loss(features, target, weights): \n","    scores = np.dot(features, weights)\n","    ll = np.sum( -target*scores + np.log(1 + np.exp(scores)) )\n","    return ll"]},{"cell_type":"markdown","metadata":{},"source":["In order to perform optimization, we need to compute the gradient and perform gradient descent. Here we have:\n","\n","$$\\nabla_{\\vec w} \\mathcal{L}(\\vec w) =  \\sum_{i = 1}^n - y_i  {\\vec x}_i  + {\\vec x}_i  \\frac{\\exp({\\vec w} \\cdot {\\vec x}_i )}{(1+\\exp({\\vec w} \\cdot {\\vec x}_i ))} = - \\sum_{i = 1}^n {\\vec x}_i^T (y_i - P(y_i=1)) $$\n","\n","We can now write the  logistic regression"]},{"cell_type":"markdown","metadata":{},"source":["##### **Exercise 1**\n","\n","##### Try writing Gradient Descent using this Loss, as we did in the previous exercise session"]},{"cell_type":"markdown","metadata":{},"source":["# 2) Logistic regression on real dataset \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Let us use the fantastic notebooks from https://physics.bu.edu/~pankajm/MLnotebooks.html, a terrific book for ML newcomers especially if they come from physics. We will consider again our friend MNIST, remember the lecture on KNN? First a bit of history on this famous dataset:\n","\n","\n","The MNIST classification problem is one of the classical ML problems for learning classification on high-dimensional data with a fairly sizable number of examples (60000). Yann LeCun and collaborators collected and processed $70000$ handwritten digits (60000 are used for training and 10000 for testing) to produce what became known as one of the most widely used datasets in ML: the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. Each handwritten digit comes in a grayscale square image in the shape of a $28\\times 28$ pixel grid. Every pixel takes a value in the range $[0,255]$, representing $256$ nuances of the gray color. The problem of image classification finds applications in a wide range of fields and is important for numerous industry applications of Machine Learning."]},{"cell_type":"markdown","metadata":{},"source":[" ### SoftMax regression:\n"," Before we consider two possible labels for our data, you can easily guess that it is amenable to have more than one in this case. \n","\n"," Do not worry! We can easily generalize what we said before. \n","\n","\n","We will use SoftMax regression, which can be thought of as a statistical model which assigns a probability that a given input image corresponds to any of the 10 handwritten digits. The model is a generalization of the logistic regression and reads:\n","$$\n","p(y_i=j|\\vec{x}_i;\\vec{w}) = \\frac{e^{-\\vec{w}_j^T \\vec{x}}}{\\sum_{k=0}^9 e^{-\\vec{w}_k^T\\vec{x} }},\n","$$\n","Where $p(y_i=j|\\vec{x}_i;\\vec{w})$ is the probability that input $\\vec{x}_i$ is the $j$-th digit, $j\\in[0,9]$.\n","The model also has 10 weight vectors $\\vec{w}_j$ which we will train below. Finally, one can use this information for prediction by taking the value of $y_i$ for which this probability is maximized:\n","\\begin{align}\n","y_{pred}=\\arg\\max_i p(y=i|\\vec{x})\n","\\end{align}\n","\n","First thing to do is to import the dataset and preprocess the data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time\n","\n","\n","from sklearn.datasets import fetch_openml # MNIST data\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils import check_random_state\n","\n","print(__doc__)\n","\n","# Turn down for faster convergence\n","train_size = 60000\n","test_size = 10000\n","\n","### load MNIST data from https://www.openml.org/d/554\n","X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["X = np.asarray(X)\n","y = np.asarray(y)"]},{"cell_type":"markdown","metadata":{},"source":["Let's plot an image to see how it looks like with plt.imshow."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(X[2,:].reshape(28,28))\n","y[2]"]},{"cell_type":"markdown","metadata":{},"source":["We shuffle the data and we do the test-train splitting."]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# shuffle data\n","random_state = check_random_state(0)\n","permutation = random_state.permutation(X.shape[0])\n","X = X[permutation]\n","y = y[permutation]\n","X = X.reshape((X.shape[0], -1))\n","# pick training and test data sets \n","X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=train_size,test_size=test_size)"]},{"cell_type":"markdown","metadata":{},"source":["We preproccess the data and use StandardScaler to have zero mean and unit variance. Pay attention that we do that only on the training set!"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["# scale data to have zero mean and unit variance [required by regressor]\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","#X_test = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["**Exercise** \n"," 1. You need to learn how to read a Python documentation. Search on the internet how to implement softmax regression.\n"," 2. Fix the value of the regularization to be $10^{-5}$. Fit the data and compute two quantities: \n","   - Sparsity of the weights (percentage of non-zero weights)\n","   - Score (i.e. accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["# 3) SVM Classification on real dataset \n"]},{"cell_type":"markdown","metadata":{},"source":["**Exercise**\n"," 1. Repeat what you did for softmax regression for an Support Vector Machine (SVM) linear classifier.\n"," 2. Go and search the Python doc for SVM classifier. Choose the linear one (we will see other in following lectures)\n"," 3. Fit the data using the default value of regularization and compare with an optimized value using CV."]},{"cell_type":"markdown","metadata":{},"source":["Clear and friendly reference can be found here: https://www.kaggle.com/nishan192/mnist-digit-recognition-using-svm. \n","\n","PS: If you do not know Kaggle, you should check it out!"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"FoIL_ex5_solved.ipynb","provenance":[{"file_id":"1fP26K7T8MbhwgQwQCtTTLYUDazsEivKx","timestamp":1634641524803},{"file_id":"1Z9BpEBbXha0Hmh2-MgMP8qQyO50tnw4A","timestamp":1633083925028}]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}
